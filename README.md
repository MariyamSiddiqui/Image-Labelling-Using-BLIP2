# ğŸ–¼ï¸ BLIP-2 Based Image Captioning and Dataset Labeling

This project demonstrates how to use the **BLIP-2 (Bootstrapping Language-Image Pre-training)** model for automatic image caption generation and dataset creation. It walks through the complete process â€” from setting up dependencies and loading images to generating captions and pushing the labeled dataset to the **Hugging Face Hub**.

---

## ğŸš€ Features
- Automatic image captioning with BLIP-2  
- Integration with Hugging Face Hub for dataset hosting  
- Efficient inference using GPU and CUDA  
- Support for 8-bit quantization with BitsAndBytes  
- Dataset creation using `Dataset.from_generator()`  
- Compatible with Google Colab  

---

## ğŸ› ï¸ Technologies & Libraries
- `transformers`  
- `datasets`  
- `bitsandbytes`  
- `accelerate`  
- `torch`  
- `PIL`  
- `requests`  
- `huggingface_hub`

---

## ğŸ“š Learning Highlights
- Understanding how BLIP-2 performs image-to-text captioning  
- Managing GPU environments and CUDA setup  
- Handling dataset uploads to Hugging Face Hub  
- Debugging Colab-specific issues (e.g., imports and memory handling)  

---

## ğŸ¯ Purpose
To explore large vision-language models for automated image labeling and dataset generation â€” providing a foundation for downstream computer vision and NLP tasks.

---

## ğŸ“¦ Output
A labeled dataset hosted on your Hugging Face account, ready for training or sharing.

---

## ğŸ‘©â€ğŸ’» Author
**Mariyam Siddiqui**  
*Machine Learning & AI Enthusiast*

---
